# Workflow for the web scraping and calculations processes

# Note: Whenever pull request is made targeting branch initialise and initialize, csv is reset

# Workflow's description
name: run-daily-scraping-and-calculations

run-name: Add scraped data and calculations to current CSV files

# Function to schedule the time (UTC), day, month, and year of the workflow process
on:
  schedule:
    - cron: "47 10 * * *"  #UTC time

jobs:
  getdataandrefreshmap:
    runs-on: ubuntu-latest
    steps:
    
    # It checks out the latest content of your repository
      - name: checkout repo content
        uses: actions/checkout@v3 # checkout the repository content to github runner.
        with:
          fetch-depth: 0
      - name: Setup python
        uses: actions/setup-python@v4
        with:
          python-version: 3.8 #install the python needed
    
    # Install the required Python dependencies
      - name: Install dependencies
        run: |
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      
      - name: execute py script 'scrape_tool'
        run: |
          python scrape_tool.py
          git status
          git config user.name github-actions
          git config user.email github-actions@github.com
          git add .
          git commit -m "crongenerated"
          git push origin HEAD:main
      
      - name: execute py script 'calculations'
        run: |
          python calculations.py
          git status
          git config user.name github-actions
          git config user.email github-actions@github.com
          git add .
          git commit -m "crongenerated"
          git push origin HEAD:main   
